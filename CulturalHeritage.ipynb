{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Adding libraries"
      ],
      "metadata": {
        "id": "HvPUZqcARokH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjDLHBvikD4N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "tfl = tf.keras.layers\n",
        "tfr = tf.keras.regularizers\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.applications import EfficientNetB2, Xception, ResNet101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dFp3LTfunntt"
      },
      "outputs": [],
      "source": [
        "num_classes = 1\n",
        "batch_size = 32\n",
        "height = width = 600"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJOvz1dBm0b1"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDKwI6HskNzB"
      },
      "outputs": [],
      "source": [
        "damaged_path = \"\"\n",
        "durable_path = \"\"\n",
        "\n",
        "def load_data(path, label):\n",
        "    X = []\n",
        "    y = []\n",
        "    for img_name in os.listdir(path):\n",
        "        img_path = os.path.join(path, img_name)\n",
        "        img = Image.open(img_path)\n",
        "        if img is not None:\n",
        "            img = img.resize((height, width), Image.LANCZOS).convert('RGB')\n",
        "            X.append(img)\n",
        "            y.append(label)\n",
        "    return X, y\n",
        "\n",
        "X_hasarli, y_hasarli = load_data(damaged_path, 1)\n",
        "X_hasarsiz, y_hasarsiz = load_data(durable_path, 0)\n",
        "\n",
        "X = np.array(X_hasarli + X_hasarsiz)\n",
        "y = np.array(y_hasarli + y_hasarsiz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kym-srmgk4B7"
      },
      "outputs": [],
      "source": [
        "shuffle_indices = np.random.permutation(len(X))\n",
        "X_shuffled = X[shuffle_indices]\n",
        "y_shuffled = y[shuffle_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsTwXyTKm9eH"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=0.25, random_state=42)\n",
        "print(\"Number of train images: \", len(X_train), \" Number of test images: \", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGbLpyS2tzZ9"
      },
      "source": [
        "## Drawing Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IljdJGcvtzH9"
      },
      "outputs": [],
      "source": [
        "def plot_hist(hist):\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
        "\n",
        "    ax1.plot(hist.history[\"accuracy\"], label=\"Training Accuracy\")\n",
        "    ax1.plot(hist.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "    ax1.set_title(\"Model Accuracy\")\n",
        "    ax1.set_ylabel(\"Accuracy\")\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(hist.history[\"loss\"], label=\"Training Loss\")\n",
        "    ax2.plot(hist.history[\"val_loss\"], label=\"Validation Loss\")\n",
        "    ax2.set_title(\"Model Loss\")\n",
        "    ax2.set_ylabel(\"Loss\")\n",
        "    ax2.set_xlabel(\"Epoch\")\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90xAif7YhXA8"
      },
      "outputs": [],
      "source": [
        "def showResults(test, pred):\n",
        "    target_names = ['positive', 'negative']\n",
        "    print(classification_report(test, pred, target_names=target_names))\n",
        "    accuracy = accuracy_score(test, pred)\n",
        "    precision=precision_score(test, pred, average='weighted')\n",
        "    f1Score=f1_score(test, pred, average='weighted')\n",
        "    print(\"Accuracy  : {}\".format(accuracy))\n",
        "    print(\"Precision : {}\".format(precision))\n",
        "    print(\"f1Score : {}\".format(f1Score))\n",
        "    cm=confusion_matrix(test, pred)\n",
        "    print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf1dMSuTuf2F"
      },
      "source": [
        "## Finding the Heat Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1lpfqLeurR7"
      },
      "outputs": [],
      "source": [
        "def visualize_gradcam(model, input_image, alpha=0.5, display=True):\n",
        "    assert 0 < alpha < 1, \"Alpha value must be between 0 and 1\"\n",
        "\n",
        "    conv_layer = next(layer for layer in reversed(model.layers) if isinstance(layer, tf.keras.layers.Conv2D))\n",
        "    target_conv_layer = model.get_layer(conv_layer.name)\n",
        "\n",
        "    processed_img = np.asarray(input_image)\n",
        "    if processed_img.ndim == 2:\n",
        "        processed_img = np.expand_dims(processed_img, axis=-1)\n",
        "    processed_img = np.expand_dims(processed_img, axis=0)\n",
        "\n",
        "    prediction = model.predict(processed_img)\n",
        "    print(\"Prediction Result:\", prediction)\n",
        "    print(\"Status:\", \"Intact\" if prediction < 0.5 else \"Damaged\")\n",
        "    predicted_class = np.argmax(prediction)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        grad_model = Model(inputs=model.input, outputs=[target_conv_layer.output, model.output])\n",
        "        conv_output, predictions = grad_model(processed_img)\n",
        "        loss = predictions[:, predicted_class]\n",
        "\n",
        "    grads = tape.gradient(loss, conv_output)\n",
        "    pooled_grads = tf.reduce_mean(grads[0], axis=(0, 1))\n",
        "\n",
        "    heatmap = np.zeros(conv_output.shape[1:3], dtype=np.float32)\n",
        "    for i, weight in enumerate(pooled_grads):\n",
        "        heatmap += weight * conv_output[0, :, :, i]\n",
        "\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap /= np.max(heatmap)\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.resize(heatmap, (processed_img.shape[2], processed_img.shape[1]))\n",
        "\n",
        "    colored_map = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    original_image = np.uint8(255 * (processed_img[0] - processed_img.min()) / (processed_img.max() - processed_img.min()))\n",
        "    blended_image = np.uint8(original_image * alpha + colored_map * (1 - alpha))\n",
        "\n",
        "    if display:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "        ax[0].imshow(original_image)\n",
        "        ax[0].axis(\"off\")\n",
        "        ax[0].set_title(\"Original Image\")\n",
        "\n",
        "        ax[1].imshow(blended_image)\n",
        "        ax[1].axis(\"off\")\n",
        "        ax[1].set_title(\"Grad-CAM Map\")\n",
        "\n",
        "        plt.show()\n",
        "    else:\n",
        "        return blended_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJegXjnjobbH"
      },
      "source": [
        "## Creating the Transfer Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz0h16IeopFB"
      },
      "outputs": [],
      "source": [
        "def build_model(num_classes,drop_prob=0.4):\n",
        "    inputs = layers.Input(shape=(height, width, 3), name='input_layer')\n",
        "\n",
        "    # Model\n",
        "    #model = EfficientNetB2(input_tensor=x, weights=\"imagenet\",include_top=False)\n",
        "    #model = ResNet101(input_tensor=inputs, weights=\"imagenet\",include_top=False)\n",
        "    model = Xception(input_tensor=inputs, weights=\"imagenet\", include_top=False)\n",
        "    model.trainable = False\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
        "    x = layers.Dropout(drop_prob)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(512, activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(drop_prob)(x)\n",
        "\n",
        "    x = layers.Dense(num_classes, activation=\"sigmoid\", name='output_layer')(x)\n",
        "    outputs = x\n",
        "\n",
        "    optimizer = keras.optimizers.Adam()\n",
        "    model = keras.Model(inputs, outputs, name=model.name)\n",
        "    model.compile(\n",
        "        optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO5GnUcuuHLY"
      },
      "source": [
        "## Education Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z48Cq_6FFNd6"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"model_checkpoint.h5\"\n",
        "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NTM1GsnTtuqx"
      },
      "outputs": [],
      "source": [
        "model = build_model(num_classes,drop_prob=0.4)\n",
        "hist = model.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Educational evaluation"
      ],
      "metadata": {
        "id": "o6tSqQzvULBQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0ciiRZ7srCk"
      },
      "outputs": [],
      "source": [
        "plot_hist(hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4iXwEbvhLuv"
      },
      "outputs": [],
      "source": [
        "yPred = model.predict(X_test)\n",
        "yTest = to_categorical(y_test)\n",
        "pred = np.argmax(yPred, axis=1)\n",
        "test = np.argmax(yTest, axis=1)\n",
        "showResults(test, pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testting"
      ],
      "metadata": {
        "id": "hzHWvDrnUN5a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clqSN0QHUH-J"
      },
      "outputs": [],
      "source": [
        "image_test_path = \"\"\n",
        "img = Image.open(image_test_path)\n",
        "img = img.resize((height,width))\n",
        "img = np.array(img)\n",
        "img = img/255.0\n",
        "visualize_gradcam(model,img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}